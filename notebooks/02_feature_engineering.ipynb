{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2b0df4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Section 1: Loading and Cleaning Data ---\n",
      "Data loaded successfully from C:\\Users\\nicol\\OneDrive\\Documentos\\customer-churn-prediction\\data\\WA_Fn-UseC_-Telco-Customer-Churn.csv\n",
      "Initial shape: (7043, 21)\n",
      "Starting data cleaning...\n",
      "Dropped 'customerID' column.\n",
      "Converted 'TotalCharges' to numeric.\n",
      "Filled missing 'TotalCharges' with median: 1397.475\n",
      "Replaced 'No internet service' with 'No'.\n",
      "Replaced 'No phone service' with 'No' in 'MultipleLines'.\n",
      "Mapped 'Churn' target variable to 1/0.\n",
      "Data cleaning complete.\n",
      "Cleaned data shape: (7043, 19)\n",
      "Target variable shape: (7043,)\n",
      "\n",
      "--- Section 2: Defining and Applying Preprocessing Pipeline ---\n",
      "Starting data preprocessing...\n",
      "Preprocessing pipeline (ColumnTransformer) created.\n",
      "Numerical features: ['SeniorCitizen', 'tenure', 'MonthlyCharges', 'TotalCharges']\n",
      "Categorical features: ['gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\n",
      "\n",
      "Preprocessor has been set up using ColumnTransformer.\n",
      "It will perform:\n",
      "  - StandardScaler on: ['SeniorCitizen', 'tenure', 'MonthlyCharges', 'TotalCharges']\n",
      "  - OneHotEncoder on: ['gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod']\n",
      "\n",
      "--- Section 3: Data Splitting and SMOTE ---\n",
      "Splitting data into training (80% and testing (20%) sets.\n",
      "Original training set shape: (5634, 38), target shape: (5634,)\n",
      "Original testing set shape: (1409, 38), target shape: (1409,)\n",
      "Applying SMOTE to balance the training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nicol\\OneDrive\\Documentos\\customer-churn-prediction\\src\\data_processor.py:76: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  self.df['TotalCharges'].fillna(median_total_charges, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data resampled with SMOTE. New shape: (8278, 38), target shape: (8278,)\n",
      "\n",
      "Shape of X_train (after preprocessing and SMOTE): (8278, 38)\n",
      "Shape of y_train (after SMOTE): (8278,)\n",
      "Shape of X_test (after preprocessing): (1409, 38)\n",
      "Shape of y_test: (1409,)\n",
      "\n",
      "Class distribution in y_train (after SMOTE):\n",
      "Churn\n",
      "0    4139\n",
      "1    4139\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Class distribution in y_test:\n",
      "Churn\n",
      "0    1035\n",
      "1     374\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total number of processed features: 38\n",
      "Example processed feature names (first 10): ['num__SeniorCitizen' 'num__tenure' 'num__MonthlyCharges'\n",
      " 'num__TotalCharges' 'cat__gender_Female' 'cat__gender_Male'\n",
      " 'cat__Partner_No' 'cat__Partner_Yes' 'cat__Dependents_No'\n",
      " 'cat__Dependents_Yes']\n",
      "\n",
      "Feature Engineering Complete. The processed data (X_train_processed, X_test_processed, y_train, y_test)\n",
      "and the preprocessor are ready for model training in the next notebook.\n"
     ]
    }
   ],
   "source": [
    "# notebooks/02_feature_engineering.ipynb\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../src')))\n",
    "\n",
    "from data_processor import DataProcessor\n",
    "\n",
    "# --- Section 1: Load and Clean Data ---\n",
    "print(\"--- Section 1: Loading and Cleaning Data ---\")\n",
    "\n",
    "# Initialize DataProcessor\n",
    "processor = DataProcessor(data_path=r\"C:\\Users\\nicol\\OneDrive\\Documentos\\customer-churn-prediction\\data\\WA_Fn-UseC_-Telco-Customer-Churn.csv\")\n",
    "processor.load_data()\n",
    "processor.clean_data() # Apply initial cleaning steps\n",
    "\n",
    "# Get the cleaned DataFrame and separate features (X) and target (y)\n",
    "df_cleaned = processor.get_raw_data()\n",
    "if df_cleaned is not None:\n",
    "    X_raw = df_cleaned.drop('Churn', axis=1)\n",
    "    y = df_cleaned['Churn']\n",
    "    print(f\"Cleaned data shape: {X_raw.shape}\")\n",
    "    print(f\"Target variable shape: {y.shape}\")\n",
    "\n",
    "# --- Section 2: Define and Apply Preprocessing Pipeline ---\n",
    "print(\"\\n--- Section 2: Defining and Applying Preprocessing Pipeline ---\")\n",
    "\n",
    "# Call preprocess_data to set up the ColumnTransformer\n",
    "processor.preprocess_data()\n",
    "\n",
    "# Identify feature types as per the data processor\n",
    "numerical_features = processor.numerical_features\n",
    "categorical_features = processor.categorical_features\n",
    "\n",
    "print(f\"Numerical features: {numerical_features}\")\n",
    "print(f\"Categorical features: {categorical_features}\")\n",
    "\n",
    "# Apply preprocessing to the raw features (X_raw)\n",
    "# The `fit_transform` method of the preprocessor will be used later during `split_data`\n",
    "# inside the DataProcessor. Here, we're just confirming the setup.\n",
    "print(\"\\nPreprocessor has been set up using ColumnTransformer.\")\n",
    "print(\"It will perform:\")\n",
    "print(f\"  - StandardScaler on: {numerical_features}\")\n",
    "print(f\"  - OneHotEncoder on: {categorical_features}\")\n",
    "\n",
    "# --- Section 3: Data Splitting and SMOTE for Imbalance Handling ---\n",
    "print(\"\\n--- Section 3: Data Splitting and SMOTE ---\")\n",
    "\n",
    "# Split data into training and test sets, and apply SMOTE to the training set\n",
    "# X_train_processed and X_test_processed will be numpy arrays after transformation\n",
    "X_train_processed, X_test_processed, y_train, y_test = processor.split_data(\n",
    "    test_size=0.2, random_state=42, apply_smote=True\n",
    ")\n",
    "\n",
    "if X_train_processed is not None:\n",
    "    print(f\"\\nShape of X_train (after preprocessing and SMOTE): {X_train_processed.shape}\")\n",
    "    print(f\"Shape of y_train (after SMOTE): {y_train.shape}\")\n",
    "    print(f\"Shape of X_test (after preprocessing): {X_test_processed.shape}\")\n",
    "    print(f\"Shape of y_test: {y_test.shape}\")\n",
    "\n",
    "    print(\"\\nClass distribution in y_train (after SMOTE):\")\n",
    "    print(y_train.value_counts())\n",
    "    print(\"\\nClass distribution in y_test:\")\n",
    "    print(y_test.value_counts())\n",
    "\n",
    "    # Get the names of the processed features (useful for model interpretability later)\n",
    "    processed_feature_names = processor.get_feature_names_out()\n",
    "    if processed_feature_names is not None:\n",
    "        print(f\"\\nTotal number of processed features: {len(processed_feature_names)}\")\n",
    "        print(\"Example processed feature names (first 10):\", processed_feature_names[:10])\n",
    "\n",
    "# --- Section 4: Save Processed Data for Next Notebook ---\n",
    "# In a real pipeline, you might save these arrays, but passing them directly\n",
    "# between notebooks in a live session is also common.\n",
    "# For reproducibility and to ensure the next notebook starts with the exact same data,\n",
    "# we'll create a quick way to pass them via global variables or save/load.\n",
    "# Since Jupyter kernel restarts clear variables, saving is safer.\n",
    "# For simplicity, we assume you're running notebooks sequentially and will pass these.\n",
    "\n",
    "# It's good practice to save these for persistence if running independently.\n",
    "# Example:\n",
    "# np.save('../data/X_train_processed.npy', X_train_processed)\n",
    "# np.save('../data/X_test_processed.npy', X_test_processed)\n",
    "# np.save('../data/y_train.npy', y_train)\n",
    "# np.save('../data/y_test.npy', y_test)\n",
    "# joblib.dump(processor.get_preprocessor(), '../models/preprocessor.pkl') # Save the preprocessor\n",
    "\n",
    "print(\"\\nFeature Engineering Complete. The processed data (X_train_processed, X_test_processed, y_train, y_test)\")\n",
    "print(\"and the preprocessor are ready for model training in the next notebook.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
